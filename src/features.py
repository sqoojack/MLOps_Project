import pandas as pd
import yaml
import os
import json
from datasets import load_dataset
from sklearn.model_selection import train_test_split

with open("params.yaml") as f:
    params = yaml.safe_load(f)

def process_data():
    print("ğŸš€ Loading Amazon Reviews 2023 from Hugging Face...")
    
    # æŒ‡å®šé¡åˆ¥ï¼Œä¾‹å¦‚ "All_Beauty" (ç¾å¦), "Fashion" (æ™‚å°š)
    # å®Œæ•´åˆ—è¡¨å¯è¦‹: https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023
    category = "All_Beauty" 
    
    # 1. è¼‰å…¥è©•è«–æ•¸æ“š (User-Item Interactions)
    # trust_remote_code=True æ˜¯å¿…é ˆçš„ï¼Œå› ç‚ºé€™æ˜¯è‡ªå®šç¾© loading script
    dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", f"raw_review_{category}", split="full", trust_remote_code=True)
    
    # è½‰ç‚º Pandas DataFrame (ç‚ºäº†æ–¹ä¾¿å¾ŒçºŒè™•ç†ï¼Œè‹¥è³‡æ–™é‡å¤ªå¤§å»ºè­°ç”¨ PyArrow)
    # é€™è£¡ç¤ºç¯„å–å‰ 10 è¬ç­†æˆ–æ˜¯ä¾ç…§è¨˜æ†¶é«”å¤§å°èª¿æ•´
    df = dataset.to_pandas()
    
    # ä¿ç•™éœ€è¦çš„æ¬„ä½
    # æ–°ç‰ˆæ¬„ä½åç¨±: rating, title, text, images, asin, parent_asin, user_id, timestamp
    df = df[['user_id', 'parent_asin', 'timestamp']]
    df.columns = ['visitorid', 'itemid', 'timestamp']
    
    # 2. å»ºç«‹ Item Map
    unique_items = df['itemid'].unique()
    item_map = {asin: i+1 for i, asin in enumerate(unique_items)}
    
    with open(params['data']['item_map_path'], 'w') as f:
        json.dump(item_map, f)
    
    df['item_idx'] = df['itemid'].map(item_map)
    
    # 3. è¼‰å…¥ Metadata (å•†å“è³‡è¨Š)
    print("ğŸ“¦ Loading Metadata...")
    meta_dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", f"raw_meta_{category}", split="full", trust_remote_code=True)
    meta_df = meta_dataset.to_pandas()
    
    metadata_map = {}
    # å»ºç«‹æŸ¥æ‰¾è¡¨
    # æ–°ç‰ˆ Metadata æ¬„ä½: title, price, average_rating, main_category, images (list)
    for _, row in meta_df.iterrows():
        asin = row['parent_asin'] # æ³¨æ„: æ–°ç‰ˆä½¿ç”¨ parent_asin ä½œç‚ºä¸»è¦ ID
        if asin in item_map:
            # å–å¾—ç¬¬ä¸€å¼µåœ– (å¤§åœ–)
            img_url = row['images']['large'][0] if row['images'] and len(row['images']['large']) > 0 else None
            
            metadata_map[str(item_map[asin])] = {
                "name": row['title'],
                "image": img_url,
                "asin": asin,
                "price": row.get('price', 'N/A')
            }

    with open(params['data']['metadata_path'], 'w') as f:
        json.dump(metadata_map, f)
        
    print(f"âœ… Metadata processed for {len(metadata_map)} items.")

    # 4. æ’åºèˆ‡åˆ†å‰² (é‚è¼¯ä¸è®Š)
    df = df.sort_values(['visitorid', 'timestamp'])
    
    item_counts = df['item_idx'].value_counts()
    valid_items = item_counts[item_counts >= params['data']['min_item_count']].index
    df = df[df['item_idx'].isin(valid_items)]

    split_idx = int(len(df) * (1 - params['data']['test_size']))
    train_df = df.iloc[:split_idx].copy()
    test_df = df.iloc[split_idx:].copy()

    os.makedirs("data/processed", exist_ok=True)
    train_df.to_csv(params['data']['processed_train_path'], index=False)
    test_df.to_csv(params['data']['processed_test_path'], index=False)
    
    print(f"ğŸ‰ Data split done. Train: {len(train_df)}, Test: {len(test_df)}")

if __name__ == "__main__":
    process_data()